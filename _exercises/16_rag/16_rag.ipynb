{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585266ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chatlas\n",
    "import dotenv\n",
    "from pyhere import here\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b21da4",
   "metadata": {},
   "source": [
    "Python has a plethora of options for working with knowledge stores\n",
    "([llama-index](https://docs.llamaindex.ai/en/stable/),\n",
    "[pinecone](https://docs.pinecone.io/reference/python-sdk), etc.). It doesn’t\n",
    "really matter which one you choose, but due to its popularity, maturity, and\n",
    "simplicity, lets demonstrate with the\n",
    "[`llama-index`](https://docs.llamaindex.ai/en/stable/) library.\n",
    "\n",
    "With `llama-index`, it’s easy to create a knowledge store from a wide variety\n",
    "of input formats, such as text files, [web\n",
    "pages](https://docs.llamaindex.ai/en/stable/examples/data_connectors/WebPageDemo/),\n",
    "and [much more](https://pypi.org/project/llama-index-readers-markitdown/).\n",
    "\n",
    "For this task, I've downloaded the notebook files in the [Polars\n",
    "Cookbook](https://github.com/escobar-west/polars-cookbook) and converted them\n",
    "to markdown. This snippet will ingest those markdown files files, embed them,\n",
    "and create a vector store `index` that is ready for\n",
    "[retrieval](https://posit-dev.github.io/chatlas/misc/RAG.html#retrieve-content).\n",
    "\n",
    "Creating the vector store index can take a while, so we write it to disk to\n",
    "persist between sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec4395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "\n",
    "polars_cookbook = here(\"data/polars-cookbook\")\n",
    "docs = SimpleDirectoryReader(polars_cookbook).load_data()\n",
    "index = VectorStoreIndex.from_documents(docs)\n",
    "\n",
    "index.storage_context.persist(\n",
    "    persist_dir=here(\"_exercises/16_rag/polars_cookbook_index\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4bd4c1",
   "metadata": {},
   "source": [
    "With our `index` now available on disk, we’re ready to implement\n",
    "`retrieve_polars_knowledge()` – a function that retrieves relevant content\n",
    "from the our Polars Cookbook knowledge store based on the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fd0a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "index_polars_cookbook = here(\"_exercises/16_rag/polars_cookbook_index\")\n",
    "storage_context = StorageContext.from_defaults(persist_dir=index_polars_cookbook)\n",
    "index = load_index_from_storage(storage_context)\n",
    "\n",
    "\n",
    "def retrieve_polars_knowledge(query: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieve relevant content from the Polars Cookbook knowledge store based on\n",
    "    the user query.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query : str\n",
    "        The user query to search for relevant Polars knowledge.\n",
    "    \"\"\"\n",
    "    retriever = index.as_retriever(similarity_top_k=5)\n",
    "    nodes = retriever.retrieve(query)\n",
    "    return \"\\n\\n\".join([f\"<excerpt>{x.text}</excerpt>\" for x in nodes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b69a9c1",
   "metadata": {},
   "source": [
    "This particular implementation retrieves the top 5 most relevant documents\n",
    "from the `index` based on the user query, but you can adjust the number of\n",
    "results by changing the `similarity_top_k` parameter. There’s no magic number\n",
    "for this parameter, but `llama-index` defaults to 2, so you may want to\n",
    "increase it if you find that the retrieved content is too sparse or not\n",
    "relevant enough.\n",
    "\n",
    "Let's try this out now with a task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\"\n",
    "How do I find all rows in a DataFrame which have the max value for count column, after grouping by ['Sp','Mt'] columns?\n",
    "\n",
    "Example 1: the following DataFrame, which I group by ['Sp','Mt']:\n",
    "\n",
    "```\n",
    "Sp Mt Value count\n",
    "0 MM1 S1 a 2\n",
    "1 MM1 S1 n **3**\n",
    "2 MM1 S3 cb **5**\n",
    "3 MM2 S3 mk **8**\n",
    "4 MM2 S4 bg **5**\n",
    "5 MM2 S4 dgd 1\n",
    "6 MM4 S2 rd 2\n",
    "7 MM4 S2 cb 2\n",
    "8 MM4 S2 uyi **7**\n",
    "```\n",
    "\n",
    "Expected output: get the result rows whose count is max in each group, like:\n",
    "\n",
    "```\n",
    "1 MM1 S1 n **3**\n",
    "2 MM1 S3 cb **5**\n",
    "3 MM2 S3 mk **8**\n",
    "4 MM2 S4 bg **5**\n",
    "8 MM4 S2 uyi **7**\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "retrieve_polars_knowledge(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0155b9d3",
   "metadata": {},
   "source": [
    "Finally, we can plug this retrieval function into a chatlas chatbot. Copy\n",
    "the task from the previous block and paste it into the chatbot to see how it\n",
    "works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ec1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = chatlas.ChatAuto(\"openai/gpt-4.1-nano\")\n",
    "\n",
    "chat.register_tool(retrieve_polars_knowledge)\n",
    "\n",
    "chat.app()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
